Performer models are attention mechanisms that substitute softmax with low-rank kernels, as defined by Katharopoulos et al. (2020) and Shen et al. (2018). These methods rely on kernels admitting explicit representations as dot-products of finite positive-feature vectors. The approaches do not aim to approximate regular attention but propose simpler and more tractable attention mechanisms, often incorporating additional constraints or trading regular with sparse attention.